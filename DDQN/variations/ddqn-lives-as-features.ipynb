{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-20T18:49:13.973225Z",
     "iopub.status.busy": "2024-12-20T18:49:13.972967Z",
     "iopub.status.idle": "2024-12-20T18:49:18.864352Z",
     "shell.execute_reply": "2024-12-20T18:49:18.863503Z",
     "shell.execute_reply.started": "2024-12-20T18:49:13.973192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ale_py\n",
      "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale_py) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale_py) (4.12.2)\n",
      "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m32.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ale_py\n",
      "Successfully installed ale_py-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ale_py\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T18:49:21.628956Z",
     "iopub.status.busy": "2024-12-20T18:49:21.628648Z",
     "iopub.status.idle": "2024-12-20T18:49:30.033432Z",
     "shell.execute_reply": "2024-12-20T18:49:30.032521Z",
     "shell.execute_reply.started": "2024-12-20T18:49:21.628927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:596: UserWarning: \u001B[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n",
      "    _register_atari_envs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n",
      "    import ale_py\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ale_py/__init__.py\", line 68, in <module>\n",
      "    register_v0_v4_envs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ale_py/registration.py\", line 178, in register_v0_v4_envs\n",
      "    _register_rom_configs(legacy_games, obs_types, versions)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ale_py/registration.py\", line 63, in _register_rom_configs\n",
      "    gymnasium.register(\n",
      "AttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n",
      "\u001B[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from matplotlib import animation\n",
    "from collections import deque, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To plot pretty figures\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T18:49:33.347250Z",
     "iopub.status.busy": "2024-12-20T18:49:33.346457Z",
     "iopub.status.idle": "2024-12-20T18:49:33.881632Z",
     "shell.execute_reply": "2024-12-20T18:49:33.880645Z",
     "shell.execute_reply.started": "2024-12-20T18:49:33.347208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = '/GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "def resize_frame(frame):\n",
    "    #frame = frame[30:-12,5:-4] # no need for cropping, we need the whole frame\n",
    "    frame = np.average(frame,axis = 2) # grayscale\n",
    "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST) # resize to 84x84, standard for Atari and DQN\n",
    "    frame = np.array(frame,dtype = np.uint8) # convert to uint8\n",
    "    return frame\n",
    "\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.frames = deque(maxlen = max_len)\n",
    "        self.actions = deque(maxlen = max_len)\n",
    "        self.rewards = deque(maxlen = max_len)\n",
    "        self.done_flags = deque(maxlen = max_len)\n",
    "        self.lives = deque(maxlen = max_len) # added for multiple lives\n",
    "\n",
    "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal, next_lives):\n",
    "        self.frames.append(next_frame)\n",
    "        self.actions.append(next_action)\n",
    "        self.rewards.append(next_frames_reward)\n",
    "        self.done_flags.append(next_frame_terminal)\n",
    "        self.lives.append(next_lives)\n",
    "\n",
    "\n",
    "def initialize_new_game(name, env, agent):\n",
    "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
    "    \n",
    "    env.reset()\n",
    "    starting_frame = resize_frame(env.step(0)[0])\n",
    "\n",
    "    dummy_action = 0\n",
    "    dummy_reward = 0\n",
    "    dummy_done = False\n",
    "    dummy_lives = 4 # added for multiple lives\n",
    "    for i in range(3):\n",
    "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done, dummy_lives)\n",
    "\n",
    "\n",
    "def make_env(name, agent):\n",
    "    gym.register_envs(ale_py) # for gymnasium\n",
    "    env = gym.make(name, render_mode = 'rgb_array')\n",
    "    return env\n",
    "\n",
    "\n",
    "def take_step(name, env, agent, score, lives, debug):\n",
    "    \n",
    "    #1 and 2: Update timesteps and save weights\n",
    "    agent.total_timesteps += 1\n",
    "    if agent.total_timesteps % 50000 == 0:\n",
    "      agent.model.save_weights('recent_weights.hdf5')\n",
    "      print('\\nWeights saved!')\n",
    "\n",
    "    #3: Take action\n",
    "    next_frame, next_frames_reward, next_frame_terminal, _, info = env.step(agent.memory.actions[-1])\n",
    "    \n",
    "    #4: Get next state\n",
    "    next_frame = resize_frame(next_frame)\n",
    "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
    "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras' goofy format of [batch_size,rows,columns,channels]\n",
    "    \n",
    "    # added for multiple lives\n",
    "    # 1) here we can include the number of lives as a feature\n",
    "    # so that the agent will take actions based on the number of lives it has\n",
    "    # if the number of lives decreases, the agent will take different actions\n",
    "    # or\n",
    "    # 2) we can make it believe each life is a different game\n",
    "    # then we presume we can finish the game with max score with only one life\n",
    "    # here the first is implemented\n",
    "    new_lives = info['lives']\n",
    "    max_lives = 4\n",
    "    lives_channel = np.full((84,84,1), new_lives / max_lives) # normalize the number of lives for better learning\n",
    "    \n",
    "    new_state = np.concatenate((new_state, lives_channel), axis=2) # add the number of lives as a channel\n",
    "\n",
    "    \n",
    "    new_state = np.expand_dims(new_state,0) #^^^\n",
    "    \n",
    "    #5: Get next action, using next state\n",
    "    next_action = agent.get_action(new_state, new_lives / max_lives)\n",
    "\n",
    "    #6: Now we add the next experience to memory\n",
    "    #agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "    \n",
    "    # we can make a penalty for losing lives and a reward for keeping them\n",
    "    if new_lives < lives:\n",
    "        next_frames_reward -= 5\n",
    "    #else:\n",
    "    #    next_frames_reward += 0.5\n",
    "    \n",
    "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal, new_lives) # added for multiple lives\n",
    "\n",
    "    #7: If game is over, return the score\n",
    "    if next_frame_terminal:\n",
    "        return (score + next_frames_reward),True, new_lives\n",
    "\n",
    "    #8: If we are trying to debug this then render\n",
    "    if debug:\n",
    "        img = env.render()\n",
    "        global frames\n",
    "        frames.append(img)\n",
    "\n",
    "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
    "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
    "        agent.learn(debug)\n",
    "\n",
    "    return (score + next_frames_reward),False, new_lives\n",
    "\n",
    "\n",
    "def play_episode(name, env, agent, debug = False):\n",
    "    initialize_new_game(name, env, agent)\n",
    "    done = False\n",
    "    score = 0\n",
    "    lives = 4 # added for multiple lives\n",
    "    while True:\n",
    "        score,done, lives = take_step(name,env,agent,score, lives, debug)\n",
    "        if done:\n",
    "            break\n",
    "    return score\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.possible_actions = possible_actions\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = .9/100000\n",
    "        self.epsilon_min = .05\n",
    "        self.gamma = .95\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.lives = starting_lives\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input((84,84,4+1))) # 4+1 - added for multiple lives\n",
    "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
    "        model.summary()\n",
    "        print('\\nAgent Initialized\\n')\n",
    "        return model\n",
    "\n",
    "    def get_action(self,state, lives):\n",
    "        \"\"\"Explore\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.sample(self.possible_actions,1)[0]\n",
    "\n",
    "        \"\"\"Do Best Acton\"\"\"\n",
    "        with tf.device(device):\n",
    "            state_with_lives = np.concatenate([state, np.array(lives).reshape(-1, 1, 1, 1)], axis=-1)\n",
    "            a_index = np.argmax(self.model.predict(state_with_lives, verbose=0))\n",
    "        return self.possible_actions[a_index]\n",
    "\n",
    "    def _index_valid(self,index):\n",
    "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def learn(self,debug = False):\n",
    "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
    "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
    "\n",
    "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions_taken = []\n",
    "        next_rewards = []\n",
    "        next_done_flags = []\n",
    "        lives = [] # added for multiple lives\n",
    "        next_lives = [] # added for multiple lives\n",
    "\n",
    "        while len(states) < 32:\n",
    "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
    "            if self._index_valid(index):\n",
    "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
    "                state = np.moveaxis(state,0,2)/255\n",
    "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
    "                next_state = np.moveaxis(next_state,0,2)/255\n",
    "\n",
    "                states.append(state)\n",
    "                next_states.append(next_state)\n",
    "                actions_taken.append(self.memory.actions[index])\n",
    "                next_rewards.append(self.memory.rewards[index+1])\n",
    "                next_done_flags.append(self.memory.done_flags[index+1])\n",
    "                lives.append(self.memory.lives[index]) # current and next number of lives\n",
    "                next_lives.append(self.memory.lives[index+1]) # added for multiple lives\n",
    "\n",
    "        states_with_lives = np.concatenate([np.array(states), np.array(lives).reshape(-1, 1, 1, 1)], axis=-1)\n",
    "        next_states_with_lives = np.concatenate([np.array(next_states), np.array(next_lives).reshape(-1, 1, 1, 1)], axis=-1)\n",
    "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
    "        with tf.device(device):\n",
    "            # changed in order to include the lives as a feature\n",
    "            labels = self.model.predict(states_with_lives, verbose=0)\n",
    "            next_state_values = self.model_target.predict(next_states_with_lives, verbose=0)\n",
    "        \n",
    "        \"\"\"Now we define our labels, or what the output should have been\n",
    "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
    "        for i in range(32):\n",
    "            action = self.possible_actions.index(actions_taken[i])\n",
    "\n",
    "            # add penalty for lost life\n",
    "            life_penalty = -5 if next_lives[i] < self.memory.lives[index] else 0\n",
    "\n",
    "            labels[i][action] = next_rewards[i] + life_penalty + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
    "\n",
    "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
    "        with tf.device(device):\n",
    "            # changed in order to include the lives as a feature\n",
    "            self.model.fit(states_with_lives,labels,batch_size = 32, epochs = 1, verbose = 0)\n",
    "\n",
    "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.learns += 1\n",
    "        \n",
    "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
    "        if self.learns % 10000 == 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            print('\\nTarget model updated')\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T18:56:16.320424Z",
     "iopub.status.busy": "2024-12-20T18:56:16.320074Z",
     "iopub.status.idle": "2024-12-20T18:56:16.589977Z",
     "shell.execute_reply": "2024-12-20T18:56:16.589110Z",
     "shell.execute_reply.started": "2024-12-20T18:56:16.320395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                        \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape               \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001B[38;5;33mConv2D\u001B[0m)                    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m32\u001B[0m)          │           \u001B[38;5;34m8,224\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)                    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m64\u001B[0m)            │          \u001B[38;5;34m32,832\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)                    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m64\u001B[0m)            │          \u001B[38;5;34m36,928\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001B[38;5;33mFlatten\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3136\u001B[0m)                │               \u001B[38;5;34m0\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)                 │       \u001B[38;5;34m1,606,144\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m)                   │           \u001B[38;5;34m2,565\u001B[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,693</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,686,693\u001B[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,693</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,686,693\u001B[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'ALE/Pacman-v5'\n",
    "\n",
    "agent = Agent(possible_actions=[0,1,2,3,4],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .0005)\n",
    "env = make_env(name,agent)\n",
    "\n",
    "last_100_avg = [-1] # worst possible score (value of the minimum score)\n",
    "scores = deque(maxlen = 100)\n",
    "max_score = -1 # in our case, it gets +1 for each pellet eaten, 0 if caught"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T18:56:21.556478Z",
     "iopub.status.busy": "2024-12-20T18:56:21.556158Z",
     "iopub.status.idle": "2024-12-20T19:06:24.638352Z",
     "shell.execute_reply": "2024-12-20T19:06:24.637098Z",
     "shell.execute_reply.started": "2024-12-20T18:56:21.556450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No weights found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/180 [00:25<1:16:16, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 0\n",
      "Steps: 493\n",
      "Duration: 25.566528797149658\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/180 [00:51<1:16:26, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1\n",
      "Steps: 493\n",
      "Duration: 25.90168261528015\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/180 [01:17<1:15:52, 25.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 2\n",
      "Steps: 493\n",
      "Duration: 25.663928031921387\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/180 [01:42<1:15:28, 25.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 3\n",
      "Steps: 493\n",
      "Duration: 25.742773056030273\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/180 [02:08<1:14:52, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 4\n",
      "Steps: 493\n",
      "Duration: 25.569393157958984\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/180 [02:34<1:14:55, 25.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 5\n",
      "Steps: 493\n",
      "Duration: 26.153747081756592\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7/180 [03:00<1:14:26, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 6\n",
      "Steps: 493\n",
      "Duration: 25.78363800048828\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/180 [03:25<1:13:42, 25.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 7\n",
      "Steps: 493\n",
      "Duration: 25.47172451019287\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 9/180 [03:51<1:13:08, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 8\n",
      "Steps: 493\n",
      "Duration: 25.561590433120728\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10/180 [04:17<1:13:14, 25.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 9\n",
      "Steps: 493\n",
      "Duration: 26.26001477241516\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n",
      "\n",
      "Episode: 10\n",
      "Steps: 493\n",
      "Duration: 26.048697233200073\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 12/180 [05:23<1:20:49, 28.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 11\n",
      "Steps: 493\n",
      "Duration: 25.68334126472473\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 13/180 [05:49<1:17:35, 27.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 12\n",
      "Steps: 493\n",
      "Duration: 25.600038290023804\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 14/180 [06:15<1:16:00, 27.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 13\n",
      "Steps: 493\n",
      "Duration: 26.54382300376892\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/180 [06:41<1:14:17, 27.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 14\n",
      "Steps: 493\n",
      "Duration: 25.94821858406067\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 16/180 [07:07<1:12:43, 26.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 15\n",
      "Steps: 493\n",
      "Duration: 25.66542339324951\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 17/180 [07:33<1:11:40, 26.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 16\n",
      "Steps: 493\n",
      "Duration: 25.855979442596436\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 18/180 [07:59<1:11:17, 26.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 17\n",
      "Steps: 493\n",
      "Duration: 26.456830978393555\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 19/180 [08:25<1:10:27, 26.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 18\n",
      "Steps: 493\n",
      "Duration: 25.909254789352417\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 20/180 [08:51<1:09:40, 26.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 19\n",
      "Steps: 493\n",
      "Duration: 25.817891359329224\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n",
      "\n",
      "Episode: 20\n",
      "Steps: 493\n",
      "Duration: 26.249985933303833\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 22/180 [09:58<1:17:02, 29.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 21\n",
      "Steps: 493\n",
      "Duration: 26.529218912124634\n",
      "Score: 6.0\n",
      "Max Score: 6.0\n",
      "Epsilon: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 22/180 [10:03<1:12:10, 27.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-a4b2a15365d9>\u001B[0m in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0mtimesteps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtotal_timesteps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mtimee\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0mscore\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplay_episode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdebug\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#set debug to true for rendering\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0mscores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscore\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mscore\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mmax_score\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-9857e1e52e5b>\u001B[0m in \u001B[0;36mplay_episode\u001B[0;34m(name, env, agent, debug)\u001B[0m\n\u001B[1;32m     88\u001B[0m     \u001B[0mscore\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m         \u001B[0mscore\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtake_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mscore\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdebug\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m             \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-9857e1e52e5b>\u001B[0m in \u001B[0;36mtake_step\u001B[0;34m(name, env, agent, score, debug)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;31m#5: Get next action, using next state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m     \u001B[0mnext_action\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;31m#6: Now we add the next experience to memory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-9857e1e52e5b>\u001B[0m in \u001B[0;36mget_action\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;34m\"\"\"Do Best Acton\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 135\u001B[0;31m             \u001B[0ma_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    136\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpossible_actions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ma_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001B[0m\n\u001B[1;32m    444\u001B[0m     ):\n\u001B[1;32m    445\u001B[0m         \u001B[0;31m# Create an iterator that yields batches of input data.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 446\u001B[0;31m         epoch_iterator = TFEpochIterator(\n\u001B[0m\u001B[1;32m    447\u001B[0m             \u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m             \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, distribute_strategy, *args, **kwargs)\u001B[0m\n\u001B[1;32m    627\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    628\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distribute_strategy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdistribute_strategy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 629\u001B[0;31m         \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    630\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDistributedDataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    631\u001B[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001B[0m in \u001B[0;36m_get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    636\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    637\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_get_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 638\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_adapter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_tf_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    639\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    640\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0menumerate_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001B[0m in \u001B[0;36mget_tf_dataset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    234\u001B[0m             \u001B[0mindices_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindices_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 236\u001B[0;31m         \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mslice_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindices_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_inputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    237\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    238\u001B[0m         \u001B[0moptions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001B[0m in \u001B[0;36mslice_inputs\u001B[0;34m(indices_dataset, inputs)\u001B[0m\n\u001B[1;32m    227\u001B[0m                     \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExternalStatePolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIGNORE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m                 )\n\u001B[0;32m--> 229\u001B[0;31m             \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_options\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mwith_options\u001B[0;34m(self, options, name)\u001B[0m\n\u001B[1;32m   2977\u001B[0m       \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mwhen\u001B[0m \u001B[0man\u001B[0m \u001B[0moption\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mset\u001B[0m \u001B[0mmore\u001B[0m \u001B[0mthan\u001B[0m \u001B[0monce\u001B[0m \u001B[0mto\u001B[0m \u001B[0ma\u001B[0m \u001B[0mnon\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mdefault\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2978\u001B[0m     \"\"\"\n\u001B[0;32m-> 2979\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_OptionsDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2980\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2981\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mcardinality\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, input_dataset, options, name)\u001B[0m\n\u001B[1;32m   4862\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options_attr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4863\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options_attr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_mutable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4864\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options_attr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options_attr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4865\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4866\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options_attr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/options.py\u001B[0m in \u001B[0;36mmerge\u001B[0;34m(self, options)\u001B[0m\n\u001B[1;32m    755\u001B[0m       \u001B[0mthe\u001B[0m \u001B[0minput\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOptions\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m     \"\"\"\n\u001B[0;32m--> 757\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0moptions_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge_options\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/options.py\u001B[0m in \u001B[0;36mmerge_options\u001B[0;34m(*options_list)\u001B[0m\n\u001B[1;32m    160\u001B[0m       \u001B[0mthis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m       \u001B[0mthat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 162\u001B[0;31m       \u001B[0mdefault\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdefault_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    163\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mthat\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m         \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/options.py\u001B[0m in \u001B[0;36m__getattribute__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    662\u001B[0m       \u001B[0;31m#                 \"Use options.deterministic instead.\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    663\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"deterministic\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 664\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mOptions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    665\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    666\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/options.py\u001B[0m in \u001B[0;36mget_fn\u001B[0;34m(option)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moption\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m       \u001B[0moption\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdefault_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0moption\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/options.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mcreate_option\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mty\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdocstring\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault_factory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mlambda\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m   \"\"\"Creates a type-checked property.\n\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "if os.path.exists('recent_weights.hdf5'):\n",
    "    agent.model.load_weights('recent_weights.hdf5')\n",
    "    agent.model_target.load_weights('recent_weights.hdf5')\n",
    "    print('\\nWeights loaded!')\n",
    "else:\n",
    "    print('\\nNo weights found')\n",
    "agent.epsilon = 0 # Set the epsilon at the value you had when you stopped training\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(180)):\n",
    "    frames = [] # Saving the frames for the gif\n",
    "    timesteps = agent.total_timesteps\n",
    "    timee = time.time()\n",
    "    score = play_episode(name, env, agent, debug = True) #set debug to true for rendering\n",
    "    scores.append(score)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "\n",
    "    print('\\nEpisode: ' + str(i))\n",
    "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
    "    print('Duration: ' + str(time.time() - timee))\n",
    "    print('Score: ' + str(score))\n",
    "    print('Max Score: ' + str(max_score))\n",
    "    print('Epsilon: ' + str(agent.epsilon))\n",
    "    \n",
    "    if i%10==0 and i!=0:\n",
    "        anim = plot_animation(frames)\n",
    "        anim.save(\"pacman{}.gif\".format(i), dpi=100, writer= animation.PillowWriter(fps=20))# Saving the gif\n",
    "        \n",
    "    if i%100==0 and i!=0:\n",
    "        last_100_avg.append(sum(scores)/len(scores))\n",
    "        plt.plot(np.arange(0,i+1,100),last_100_avg)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "agent.model.save_weights('recent_weights.hdf5')\n",
    "agent.model_target.save_weights('recent_weights_target.hdf5')\n",
    "print('\\nWeights saved!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
