{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from matplotlib import animation\n",
    "from collections import deque, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To plot pretty figures\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T22:25:53.176664Z",
     "iopub.execute_input": "2024-12-20T22:25:53.176907Z",
     "iopub.status.idle": "2024-12-20T22:26:02.832853Z",
     "shell.execute_reply.started": "2024-12-20T22:25:53.176887Z",
     "shell.execute_reply": "2024-12-20T22:26:02.832159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:596: UserWarning: \u001B[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n    fn()\n  File \"/usr/local/lib/python3.10/dist-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n    _register_atari_envs()\n  File \"/usr/local/lib/python3.10/dist-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n    import ale_py\n  File \"/usr/local/lib/python3.10/dist-packages/ale_py/__init__.py\", line 68, in <module>\n    register_v0_v4_envs()\n  File \"/usr/local/lib/python3.10/dist-packages/ale_py/registration.py\", line 178, in register_v0_v4_envs\n    _register_rom_configs(legacy_games, obs_types, versions)\n  File \"/usr/local/lib/python3.10/dist-packages/ale_py/registration.py\", line 63, in _register_rom_configs\n    gymnasium.register(\nAttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n\u001B[0m\n  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "device = '/GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "def resize_frame(frame):\n",
    "    #frame = frame[30:-12,5:-4] # no need for cropping, we need the whole frame\n",
    "    frame = np.average(frame,axis = 2) # grayscale\n",
    "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST) # resize to 84x84, standard for Atari and DQN\n",
    "    frame = np.array(frame,dtype = np.uint8) # convert to uint8\n",
    "    return frame\n",
    "\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.frames = deque(maxlen = max_len)\n",
    "        self.actions = deque(maxlen = max_len)\n",
    "        self.rewards = deque(maxlen = max_len)\n",
    "        self.done_flags = deque(maxlen = max_len)\n",
    "        #self.lives = deque(maxlen = max_len) # added for multiple lives\n",
    "\n",
    "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal, next_lives):\n",
    "        self.frames.append(next_frame)\n",
    "        self.actions.append(next_action)\n",
    "        self.rewards.append(next_frames_reward)\n",
    "        self.done_flags.append(next_frame_terminal)\n",
    "        #self.lives.append(next_lives)\n",
    "\n",
    "\n",
    "def initialize_new_game(name, env, agent):\n",
    "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
    "    \n",
    "    env.reset()\n",
    "    starting_frame = resize_frame(env.step(0)[0])\n",
    "\n",
    "    dummy_action = 0\n",
    "    dummy_reward = 0\n",
    "    dummy_done = False\n",
    "    #dummy_lives = 4 # added for multiple lives\n",
    "    for i in range(3):\n",
    "        #agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done, dummy_lives)\n",
    "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
    "\n",
    "\n",
    "def make_env(name, agent):\n",
    "    gym.register_envs(ale_py) # for gymnasium\n",
    "    env = gym.make(name, render_mode = 'rgb_array')\n",
    "    return env\n",
    "\n",
    "\n",
    "def take_step(name, env, agent, score, lives, debug):\n",
    "    \n",
    "    #1 and 2: Update timesteps and save weights\n",
    "    agent.total_timesteps += 1\n",
    "    if agent.total_timesteps % 50000 == 0:\n",
    "      agent.model.save_weights('recent.weights.h5')\n",
    "      print('\\nWeights saved!')\n",
    "\n",
    "    #3: Take action\n",
    "    next_frame, next_frames_reward, next_frame_terminal, _, info = env.step(agent.memory.actions[-1])\n",
    "    \n",
    "    #4: Get next state\n",
    "    next_frame = resize_frame(next_frame)\n",
    "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
    "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras' goofy format of [batch_size,rows,columns,channels]\n",
    "    \n",
    "    # added for multiple lives\n",
    "    # 1) here we can include the number of lives as a feature\n",
    "    # so that the agent will take actions based on the number of lives it has\n",
    "    # if the number of lives decreases, the agent will take different actions\n",
    "    # or\n",
    "    # 2) we can make it believe each life is a different game\n",
    "    # then we presume we can finish the game with max score with only one life\n",
    "    # but also it would make sense since it gets respawned each time it dies\n",
    "    # so it must learn anyways to deal with that state\n",
    "    # here the second is implemented\n",
    "    new_lives = info['lives']\n",
    "    max_lives = 4\n",
    "    #lives_channel = np.full((84,84,1), new_lives / max_lives) # normalize the number of lives for better learning\n",
    "    \n",
    "    #new_state = np.concatenate((new_state, lives_channel), axis=2) # add the number of lives as a channel\n",
    "\n",
    "    \n",
    "    new_state = np.expand_dims(new_state,0) #^^^\n",
    "    \n",
    "    #5: Get next action, using next state\n",
    "    next_action = agent.get_action(new_state, new_lives / max_lives)\n",
    "\n",
    "    #6: Now we add the next experience to memory\n",
    "    #agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "    \n",
    "    # we can make a penalty for losing lives and a reward for keeping them\n",
    "    #if new_lives < lives:\n",
    "    #    next_frames_reward -= 5\n",
    "    #else:\n",
    "    #    next_frames_reward += 0.5\n",
    "    \n",
    "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal, new_lives) # added for multiple lives\n",
    "\n",
    "    #7: If game is over, return the score\n",
    "    if next_frame_terminal:\n",
    "        return (score + next_frames_reward),True, new_lives\n",
    "\n",
    "    #8: If we are trying to debug this then render\n",
    "    if debug:\n",
    "        img = env.render()\n",
    "        global frames\n",
    "        frames.append(img)\n",
    "\n",
    "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
    "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
    "        agent.learn(debug)\n",
    "\n",
    "    return (score + next_frames_reward),False, new_lives\n",
    "\n",
    "\n",
    "def play_episode(name, env, agent, debug = False):\n",
    "    initialize_new_game(name, env, agent)\n",
    "    done = False\n",
    "    score = 0\n",
    "    lives = 4 # added for multiple lives\n",
    "    prev_lives = 4\n",
    "    while True:\n",
    "        score,done, lives = take_step(name,env,agent,score, lives, debug)\n",
    "        if lives != prev_lives or done:\n",
    "            break\n",
    "    return score\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.possible_actions = possible_actions\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = .9/10000\n",
    "        self.epsilon_min = .05\n",
    "        self.gamma = .95\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.lives = starting_lives\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input((84,84,4))) # 4+1 - added for multiple lives\n",
    "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
    "        model.summary()\n",
    "        print('\\nAgent Initialized\\n')\n",
    "        return model\n",
    "\n",
    "    def get_action(self,state, normalized_lives):\n",
    "        \"\"\"Explore\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.possible_actions)\n",
    "\n",
    "        \"\"\"Do Best Acton\"\"\"\n",
    "        with tf.device(device):\n",
    "            #normalized_lives_tensor = np.full((state.shape[0], state.shape[1], 1), normalized_lives)\n",
    "            #state_with_lives = np.concatenate([state, normalized_lives_tensor], axis=2)\n",
    "            #a_index = np.argmax(self.model.predict(np.expand_dims(state_with_lives,0), verbose=0))\n",
    "            a_index = np.argmax(self.model.predict(state, verbose=0))\n",
    "        return self.possible_actions[a_index]\n",
    "\n",
    "    def _index_valid(self,index):\n",
    "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def learn(self,debug = False):\n",
    "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
    "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
    "\n",
    "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions_taken = []\n",
    "        next_rewards = []\n",
    "        next_done_flags = []\n",
    "        lives = [] # added for multiple lives\n",
    "        next_lives = [] # added for multiple lives\n",
    "\n",
    "        while len(states) < 32:\n",
    "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
    "            if self._index_valid(index):\n",
    "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
    "                state = np.moveaxis(state,0,2)/255\n",
    "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
    "                next_state = np.moveaxis(next_state,0,2)/255\n",
    "\n",
    "                states.append(state)\n",
    "                next_states.append(next_state)\n",
    "                actions_taken.append(self.memory.actions[index])\n",
    "                next_rewards.append(self.memory.rewards[index+1])\n",
    "                next_done_flags.append(self.memory.done_flags[index+1])\n",
    "                # lives.append(self.memory.lives[index]) # current and next number of lives\n",
    "                # next_lives.append(self.memory.lives[index+1]) # added for multiple lives\n",
    "\n",
    "        #states_with_lives = np.concatenate([np.array(states), np.array(lives).reshape(-1, 1, 1, 1)], axis=-1)\n",
    "        #next_states_with_lives = np.concatenate([np.array(next_states), np.array(next_lives).reshape(-1, 1, 1, 1)], axis=-1)\n",
    "\n",
    "        # lives_channels = np.expand_dims(np.array(lives), axis=(1, 2))\n",
    "        # lives_channels = np.repeat(lives_channels, 84, axis=1)\n",
    "        # lives_channels = np.repeat(lives_channels, 84, axis=2)\n",
    "        # lives_channels = np.expand_dims(lives_channels, axis=-1)\n",
    "        # states_with_lives = np.concatenate((np.array(states), lives_channels), axis=-1)\n",
    "        # \n",
    "        # next_lives_channels = np.expand_dims(np.array(next_lives), axis=(1, 2))\n",
    "        # next_lives_channels = np.repeat(next_lives_channels, 84, axis=1)\n",
    "        # next_lives_channels = np.repeat(next_lives_channels, 84, axis=2)\n",
    "        # next_lives_channels = np.expand_dims(next_lives_channels, axis=-1)\n",
    "        # next_states_with_lives = np.concatenate((np.array(next_states), next_lives_channels), axis=-1)\n",
    "        \n",
    "        \n",
    "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
    "        with tf.device(device):\n",
    "            # changed in order to include the lives as a feature\n",
    "            #labels = self.model.predict(states_with_lives, verbose=0)\n",
    "            #next_state_values = self.model_target.predict(next_states_with_lives, verbose=0)\n",
    "            labels = self.model.predict(states, verbose=0)\n",
    "            next_state_values = self.model_target.predict(next_states, verbose=0)\n",
    "        \n",
    "        \"\"\"Now we define our labels, or what the output should have been\n",
    "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
    "        for i in range(32):\n",
    "            action = self.possible_actions.index(actions_taken[i])\n",
    "\n",
    "            # add penalty for lost life\n",
    "            #life_penalty = -5 if next_lives[i] < self.memory.lives[index] else 0\n",
    "            # no more penalty if condidered as a separate game\n",
    "\n",
    "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
    "\n",
    "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
    "        with tf.device(device):\n",
    "            # changed in order to include the lives as a feature\n",
    "            #self.model.fit(states_with_lives,labels,batch_size = 32, epochs = 1, verbose = 0)\n",
    "            self.model.fit(states, labels, batch_size = 32, epochs = 1, verbose = 0)\n",
    "\n",
    "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.learns += 1\n",
    "        \n",
    "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
    "        if self.learns % 10000 == 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            print('\\nTarget model updated')\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T22:34:38.258242Z",
     "iopub.execute_input": "2024-12-20T22:34:38.258719Z",
     "iopub.status.idle": "2024-12-20T22:34:38.298694Z",
     "shell.execute_reply.started": "2024-12-20T22:34:38.258682Z",
     "shell.execute_reply": "2024-12-20T22:34:38.297797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "name = 'ALE/Pacman-v5'\n\n#agent = Agent(possible_actions=[0,1,2,3,4],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .0005)\nagent = Agent(possible_actions=[0,1,2,3,4],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .0005)\nenv = make_env(name,agent)\n\nlast_100_avg = [-21] # worst possible score (value of the minimum score)\nscores = deque(maxlen = 100)\nmax_score = -21 # in our case, it gets +1 for each pellet eaten, 0 if caught",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T22:34:39.289861Z",
     "iopub.execute_input": "2024-12-20T22:34:39.290143Z",
     "iopub.status.idle": "2024-12-20T22:34:39.556715Z",
     "shell.execute_reply.started": "2024-12-20T22:34:39.290122Z",
     "shell.execute_reply": "2024-12-20T22:34:39.555833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential_5\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                        \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape               \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_15 (\u001B[38;5;33mConv2D\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m32\u001B[0m)          │          \u001B[38;5;34m10,272\u001B[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_16 (\u001B[38;5;33mConv2D\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m64\u001B[0m)            │          \u001B[38;5;34m32,832\u001B[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_17 (\u001B[38;5;33mConv2D\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m64\u001B[0m)            │          \u001B[38;5;34m36,928\u001B[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_5 (\u001B[38;5;33mFlatten\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3136\u001B[0m)                │               \u001B[38;5;34m0\u001B[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (\u001B[38;5;33mDense\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)                 │       \u001B[38;5;34m1,606,144\u001B[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (\u001B[38;5;33mDense\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m)                   │           \u001B[38;5;34m2,565\u001B[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,272</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,688,741\u001B[0m (6.44 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,688,741</span> (6.44 MB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,688,741\u001B[0m (6.44 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,688,741</span> (6.44 MB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nAgent Initialized\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "env.reset()\n\nif os.path.exists('recent_weights.hdf5'):\n    agent.model.load_weights('recent_weights.hdf5')\n    agent.model_target.load_weights('recent_weights.hdf5')\n    print('\\nWeights loaded!')\nelse:\n    print('\\nNo weights found')\n# only when previously trained!\n#agent.epsilon = 0 # Set the epsilon at the value you had when you stopped training\n\n\n\nfor i in tqdm(range(1000)):\n    frames = [] # Saving the frames for the gif\n    timesteps = agent.total_timesteps\n    timee = time.time()\n    score = play_episode(name, env, agent, debug = True) #set debug to true for rendering\n    scores.append(score)\n    if score > max_score:\n        max_score = score\n\n    if i%50==0:\n        print('\\nEpisode: ' + str(i))\n        print('Steps: ' + str(agent.total_timesteps - timesteps))\n        print('Duration: ' + str(time.time() - timee))\n        print('Score: ' + str(score))\n        print('Max Score: ' + str(max_score))\n        print('Epsilon: ' + str(agent.epsilon))\n        \n        print('Avg Weights: ' + str(np.mean(agent.model.get_weights()[0])))\n        #print(agent.model.get_weights()[0])\n    \n    if i%50==0 and i!=0:\n        anim = plot_animation(frames)\n        anim.save(\"pacman{}.gif\".format(i), dpi=100, writer= animation.PillowWriter(fps=20))# Saving the gif\n        \n    if i%100==0 and i!=0:\n        last_100_avg.append(sum(scores)/len(scores))\n        plt.plot(np.arange(0,i+1,100),last_100_avg)\n        plt.show()\n\n\nagent.model.save_weights('recent.weights.h5')\nagent.model_target.save_weights('recent_target.weights.h5')\nprint('\\nWeights saved!')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T22:34:42.993760Z",
     "iopub.execute_input": "2024-12-20T22:34:42.994055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nNo weights found\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  0%|          | 1/1000 [00:00<16:25,  1.01it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEpisode: 0\nSteps: 505\nDuration: 0.9780395030975342\nScore: 3.0\nMax Score: 3.0\nEpsilon: 1\nAvg Weights: 0.00022293274\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  4%|▍         | 39/1000 [00:33<12:17,  1.30it/s]",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
