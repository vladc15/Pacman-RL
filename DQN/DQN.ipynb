{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-08T16:07:05.442261Z",
     "start_time": "2025-01-08T16:06:47.034036Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "!pip install ale_py\n",
    "import ale_py\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from gymnasium import wrappers\n",
    "from tqdm import tqdm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\v\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale_py in c:\\users\\v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\users\\v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ale_py) (1.26.4)\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T16:07:09.543569Z",
     "start_time": "2025-01-08T16:07:08.709974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size, load_model=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        if load_model: # loading the model will disable training\n",
    "            self.discount = 0.99\n",
    "            self.learning_rate = 0.00001\n",
    "            self.epsilon = 0.2 # exploring at 1 and exploiting at 0\n",
    "            self.epsilon_decay = 0.99999\n",
    "            self.epsilon_min = 0.2\n",
    "        else:\n",
    "            self.discount = 0.99\n",
    "            self.learning_rate = 0.001\n",
    "            self.epsilon = 1.0\n",
    "            self.epsilon_decay = 0.9999\n",
    "            self.epsilon_min = 0.1\n",
    "            \n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        if load_model:\n",
    "            self.model.load_weights('./training-dqn/pacman.h5')\n",
    "            \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        #model.add(Input(shape=(self.state_size,)))\n",
    "        model.add(Input(shape=(84, 84, 1)))\n",
    "        \n",
    "        #model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        #model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        \"\"\"model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2,2), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\"\"\"\n",
    "        \n",
    "        \"\"\"model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.action_size), activation = 'linear'))\"\"\"\n",
    "        \n",
    "        model.add(Conv2D(32, (8, 8), strides=4, activation='relu', kernel_initializer='he_uniform', ))\n",
    "        model.add(Conv2D(64, (4, 4), strides=2, activation='relu', kernel_initializer='he_uniform', ))\n",
    "        model.add(Conv2D(64, (3, 3), strides=1, activation='relu', kernel_initializer='he_uniform', ))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with tf.device(device):\n",
    "                act_values = self.model.predict(state, verbose=0) # get the Q-values for the state\n",
    "            return np.argmax(act_values[0]) # return the action with the highest Q-value\n",
    "        \n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        # if self.epsilon > self.epsilon_min:\n",
    "        #     self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def experience_replay(self): # experience replay\n",
    "        # if len(self.memory) < self.train_start:\n",
    "        #     return\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        #batch_size = min(len(self.memory), self.batch_size)\n",
    "        batch_size = self.batch_size\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        #state = np.zeros((batch_size, self.state_size))\n",
    "        #next_state = np.zeros((batch_size, self.state_size))\n",
    "        state = np.zeros((batch_size, 84, 84, 1))\n",
    "        next_state = np.zeros((batch_size, 84, 84, 1))\n",
    "        \n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            state[i] = minibatch[i][0].reshape((84, 84, 1))\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3].reshape((84, 84, 1))\n",
    "            done.append(minibatch[i][4])\n",
    "            \n",
    "        with tf.device(device):\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            target_next = self.model.predict(next_state, verbose=0)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount * np.amax(target_next[i])\n",
    "        with tf.device(device):\n",
    "            self.model.fit(state, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ],
   "id": "8b345dda8316c194",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T16:07:10.493031Z",
     "start_time": "2025-01-08T16:07:10.265247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 1200\n",
    "environment = 'ALE/Pacman-v5'\n",
    "path_training = './DQN/training-dqn/'\n",
    "path_training = '/kaggle/working/'\n",
    "gym.register_envs(ale_py)\n",
    "device = '/GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/CPU:0'"
   ],
   "id": "1fe24eb44278346b",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T16:07:11.301213Z",
     "start_time": "2025-01-08T16:07:11.220219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Pacman:\n",
    "    def __init__(self, mode, view):\n",
    "        self.env = gym.make(environment, frameskip=1, render_mode=view) # rgb_array or human\n",
    "        self.env.reset()\n",
    "        size_state = self.env.observation_space.shape[0]\n",
    "        size_action = self.env.action_space.n\n",
    "        \n",
    "        # network construction\n",
    "        if mode.lower() == 'test':\n",
    "            load_model = True\n",
    "        else:\n",
    "            load_model = False\n",
    "            \n",
    "        #self.agent = DQN(size_state, size_action, load_model)\n",
    "        self.agent = DQN((84, 84, 1), size_action, load_model)\n",
    "        \n",
    "    def train(self, path, statistics, mode):\n",
    "        if path:\n",
    "            train_path = path\n",
    "        else:\n",
    "            train_path = path_training\n",
    "            \n",
    "        if statistics:\n",
    "            print('Scores will be plotted')\n",
    "        else:\n",
    "            print('Scores will not be plotted')\n",
    "            \n",
    "        env = self.env\n",
    "        env = wrappers.AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, screen_size=84, )\n",
    "        \n",
    "        agent = self.agent\n",
    "        \n",
    "        size_state = self.env.observation_space.shape[0]\n",
    "        scores, episodes = [], []\n",
    "        \n",
    "        for e in tqdm(range(n_episodes)):\n",
    "            done = False\n",
    "            score = 0\n",
    "            state, _ = env.reset()\n",
    "            #state = np.reshape(state, [1, size_state])\n",
    "            state = np.expand_dims(state, axis=-1) # 84x84x1 now\n",
    "            state = np.moveaxis(state, 2, 0)\n",
    "            \n",
    "            lives = 4\n",
    "            while not done:\n",
    "                dead = False\n",
    "                while not dead:\n",
    "                    action = agent.get_action(state)\n",
    "                    next_state, reward, done, _, info = env.step(action)\n",
    "                    #next_state = np.reshape(next_state, [1, size_state])\n",
    "                    next_state = np.expand_dims(next_state, axis=-1)\n",
    "                    next_state = np.moveaxis(next_state, 2, 0)\n",
    "\n",
    "                    \n",
    "                    reward = reward if not dead else -10 # penalize deaths\n",
    "                    \n",
    "                    agent.append_memory(state, action, reward, next_state, done)\n",
    "                    agent.experience_replay()\n",
    "                    \n",
    "                    \n",
    "                    state = next_state\n",
    "                    score += reward\n",
    "                    dead = info['lives'] < lives\n",
    "                    lives = info['lives']\n",
    "                    #reward = reward if not dead else -10 # penalize deaths\n",
    "                    \n",
    "                if done:\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    #agent.update_target_model()\n",
    "                    \n",
    "                    if statistics:\n",
    "                        pylab.plot(episodes, scores, 'b')\n",
    "                        pylab.savefig(train_path + 'pacman.png')\n",
    "                    print('episode: {}/{}, score: {}, epsilon: {:.2}'.format(e, episodes, score, agent.epsilon))\n",
    "         \n",
    "            if e%50 == 0 and mode.lower() == 'train': # save the model every 50 episodes\n",
    "                agent.model.save_weights(train_path + 'pacman.weights.h5')\n",
    "                print('Model saved')"
   ],
   "id": "569a7e532cf5fcef",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T16:07:18.675795Z",
     "start_time": "2025-01-08T16:07:11.798381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run the code\n",
    "pacman = Pacman('train', 'rgb_array')"
   ],
   "id": "393ba7a12144fcc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_32\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_32\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_75 (\u001B[38;5;33mConv2D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │         \u001B[38;5;34m2,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_76 (\u001B[38;5;33mConv2D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m64\u001B[0m)       │        \u001B[38;5;34m32,832\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_77 (\u001B[38;5;33mConv2D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m64\u001B[0m)       │        \u001B[38;5;34m36,928\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_26 (\u001B[38;5;33mFlatten\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3136\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │     \u001B[38;5;34m1,606,144\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m)              │         \u001B[38;5;34m2,565\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,680,549\u001B[0m (6.41 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,680,549</span> (6.41 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,680,549\u001B[0m (6.41 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,680,549</span> (6.41 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T16:08:04.994427Z",
     "start_time": "2025-01-08T16:07:19.978114Z"
    }
   },
   "cell_type": "code",
   "source": "pacman.train(path=path_training, statistics=True, mode='train')",
   "id": "382ac1ebbab4f144",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores will be plotted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001B[1mCannot take the length of shape with unknown rank.\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[150], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mpacman\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_training\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstatistics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[148], line 49\u001B[0m, in \u001B[0;36mPacman.train\u001B[1;34m(self, path, statistics, mode)\u001B[0m\n\u001B[0;32m     47\u001B[0m dead \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dead:\n\u001B[1;32m---> 49\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     next_state, reward, done, _, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;66;03m#next_state = np.reshape(next_state, [1, size_state])\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[146], line 68\u001B[0m, in \u001B[0;36mDQN.get_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[1;32m---> 68\u001B[0m         act_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# get the Q-values for the state\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margmax(act_values[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling Sequential.call().\n\n\u001B[1mCannot take the length of shape with unknown rank.\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ce4560fc83c152e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
